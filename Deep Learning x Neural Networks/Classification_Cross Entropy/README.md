## Cross-Entropy?
Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.ry Cross Entropy is the negative average of the log of corrected predicted probabilities.
 . Entropy is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.
## When fitting a neural network for classification, Keras provide the following three different types of cross entropy loss function:
1. binary_crossentropy
2. categorical_crossentropy
3. sparse_categorical_crossentropy
## Binary Classification
Binary Classification is a problem where we have to segregate our observations in any of the two labels on the basis of the features. 
